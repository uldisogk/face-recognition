{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/davidsandberg/facenet  \n",
    "\n",
    "https://github.com/nyoki-mtl/keras-facenet  \n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'input_1:0' shape=(None, 160, 160, 3) dtype=float32>]\n",
      "[<tf.Tensor 'Bottleneck_BatchNorm/cond/Identity:0' shape=(None, 128) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uldis/Documents/Programming/virtual-environments/face-recognition/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# Load face encoding model - FaceNet.\n",
    "facenet_model = load_model('facenet_keras_model.h5')\n",
    "# Summarize input and output shape\n",
    "print(facenet_model.inputs)\n",
    "print(facenet_model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(filepath, mode=\"RGB\"):\n",
    "    \"\"\"\n",
    "    Function to read image from file and convert it to RGB by default, or leaves BGR.\n",
    "    Input: file path.\n",
    "    Output: image as numpy array.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath)\n",
    "    if mode == \"RGB\":\n",
    "        # Convert to RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(image, required_size=(160, 160)):\n",
    "    \"\"\"\n",
    "    Function to resize image.\n",
    "    Input: image as numpy array.\n",
    "    Param required_size: required_size as tuple (x,x); default is 160x160.\n",
    "    Output: image as numpy array.\n",
    "    \"\"\"\n",
    "    img = cv2.resize(image, required_size, interpolation=cv2.INTER_AREA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewhiten_img(x):\n",
    "    \"\"\"\n",
    "    Funtion to subtract the mean and normalize the range of the pixel values of input images.\n",
    "    Input: either one image or list of images as numpy array.\n",
    "    Output: numpy array.\n",
    "    \"\"\"\n",
    "    # For list of images.\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    # For single image.\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "    std_adj = np.maximum(std, 1.0 / np.sqrt(size))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(x, axis=-1, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    L2 normalization function to normalize 128D face embedding.\n",
    "    Input: numpy array.\n",
    "    Output: numpy array.\n",
    "    \"\"\"\n",
    "    output = x / np.sqrt(np.maximum(np.sum(np.square(x), axis=axis, keepdims=True), epsilon))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(face_image, model=facenet_model):\n",
    "    \"\"\"\n",
    "    Given an image, return the 128-dimension face encoding.\n",
    "    Input: image as numpy array of shape (160, 160, 3).\n",
    "    Param model: face embedding model; built-in (default) is FaceNet Keras model.\n",
    "    Output: numpy array of shape (1, 128)\n",
    "    \"\"\"\n",
    "    # Resize image to 160x160 (default dimensions applicable for FaceNet model).\n",
    "    img = resize_img(face_image)\n",
    "    # Normalize face image.\n",
    "    img = prewhiten_img(img)\n",
    "    # Exapand dimensions from 3 to 4 as model expects list of samples (4D array).\n",
    "    sample = np.expand_dims(img, axis=0)\n",
    "    # Make prediction to get embedding\n",
    "    pred = model.predict(sample)\n",
    "    # Normalize 128D output.\n",
    "    pred = l2_normalize(pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(face_encodings, face_to_compare):\n",
    "    \"\"\"\n",
    "    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n",
    "    for each comparison face. The distance tells you how similar the faces are.\n",
    "    Input face_encodings: List of face encodings to compare. Should be numpy array of shape (1, 128) or list of them.\n",
    "    Input face_to_compare: A face encoding to compare against. Should be numpy array of shape (1, 128)\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(face_encodings - face_to_compare, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load image path and label into df.\n",
    "# Faces of one person should be placed in one subdirectory. Subdirectory should be named after the person.\n",
    "def load_face_images(directory, df):\n",
    "    i = 0\n",
    "    for subdir in os.listdir(directory):\n",
    "        path = directory + '/' + subdir + '/'\n",
    "        if not os.path.isdir(path):\n",
    "            continue\n",
    "        for filename in os.listdir(path):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "                df.loc[i, \"label\"] = subdir\n",
    "                df.loc[i, \"img_path\"] = path + filename\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.4311442], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path1 = \"/Users/uldis/Documents/Programming/python-projects/face-recognition/examples/faces/workplace_1.jpg\"\n",
    "img_path2 = \"/Users/uldis/Documents/Programming/python-projects/face-recognition/examples/faces/workplace_4.jpg\"\n",
    "emb1 = get_embedding(read_img(img_path1))\n",
    "emb2 = get_embedding(read_img(img_path2))\n",
    "calculate_distance(emb1, emb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pandas DataFrame with faces - img path, label, encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty pandas DataFrame to store source data.\n",
    "df_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face image path and corresponding label from source dir.\n",
    "source_dir = \"\"\n",
    "load_face_images(source_dir, df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data.\n",
    "df_data = shuffle(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.shape)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the number of each person in you df.\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.countplot(df_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get face embeddings.\n",
    "df_data[\"embedding\"] = df_data.img_path.apply(lambda x: get_embedding(resize_img(read_img(x)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data to pickle file for later use.\n",
    "df_data.to_pickle(\"./faces-df.pkl\")\n",
    "# Load data from pickle file.\n",
    "# df_data = pd.read_pickle(\"./faces-df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (SVM) classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X = np.asarray(list(df_data.embedding))\n",
    "# label encode targets\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df_data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "s = 700\n",
    "X_train = X[:s]\n",
    "X_test = X[s:]\n",
    "y_train = y[:s]\n",
    "y_test = y[s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionaly) Use GridSearchCV to search for best params.\n",
    "estimator = SVC(probability=True)\n",
    "params = {\"C\": [0.01, 0.03, 0.1, 0.3, 1, 3, 10], \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\n",
    "\n",
    "clf = GridSearchCV(estimator, params, scoring='f1_micro', n_jobs=-1, cv=5, verbose=True)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model (with best parameters) and make predictions\n",
    "SupVecClas = SVC(C=3, kernel=\"linear\", probability=True)\n",
    "SupVecClas.fit(X_train, y_train)\n",
    "# Prediction\n",
    "y_pred = SupVecClas.predict(X_test)\n",
    "SupVecClas.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilities predictions. Max probability can differ from prediction.\n",
    "y_prob = SupVecClas.predict_proba(X_test)\n",
    "best_class_indices = np.argmax(y_prob, axis=1)\n",
    "best_class_probabilities = y_prob[np.arange(len(best_class_indices)), best_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(metrics.classification_report(y, SupVecClas.predict(X), target_names = label_encoder.classes_.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict person from sample face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_person(sample_img):\n",
    "    embedding = get_embedding(resize_img(read_img(sample_img)))\n",
    "    prediction = SupVecClas.predict(embedding)\n",
    "    prediction = label_encoder.inverse_transform(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"\"\n",
    "print(\"Person in the image is {}.\".format(predict_person(img_path)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify and move new face images to relevant folder using trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dir = \"\"\n",
    "dest_dir = \"\"\n",
    "for filename in os.listdir(face_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        s = os.path.join(face_dir, filename)\n",
    "        person = predict_person(s)[0]\n",
    "        d = os.path.join(dest_dir, person, filename)\n",
    "        copyfile(s, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
